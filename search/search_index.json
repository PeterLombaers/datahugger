{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Datahugger - Where DOI \ud83d\udc50 Data","text":"<p>Datahugger is a tool to download scientific datasets, software, and code from a large number of repositories based on their DOI (wiki) or URL. With Datahugger, you can automate the downloading of data and improve the reproducibility of your research. Datahugger provides a straightforward Python interface as well as an intuitive Command Line Interface (CLI).</p> <p> </p>"},{"location":"#alternatives","title":"Alternatives","text":"<p>Datahugger's main focus is on downloading datasets from arbitrary DOI's. As far as I know, there are alternatives for downloading datasets from arbitrary DOI's. However, there are many libraries for downloading datasets from a repository right away. Usually they can also be used for uploading data to the respotory. Nice examples are:</p> <ul> <li>https://guides.dataverse.org/en/latest/api/client-libraries.html</li> <li>https://pypi.org/project/pyzenodo3/</li> <li>https://osfclient.readthedocs.io/en/latest/</li> </ul>"},{"location":"development/","title":"Development","text":""},{"location":"development/#add-new-service","title":"Add new service","text":"<p>Support for repositories can be achieved by implementing a \"service\". The file datahugger/services.py list various services. For the new service, one needs to develop a new class, ideally inherited from the <code>BaseRepoDownloader</code> class. The class of Open Science Framework (<code>OSFDataset</code>) is a good example of a simple implementation.</p> <pre><code>from datahugger.base import DatasetDownloader\nfrom datahugger.base import DatasetResult\n\nclass OSFDataset(DatasetDownloader, DatasetResult):\n\"\"\"Downloader for OSF repository.\"\"\"\n\n    REGEXP_ID = r\"osf\\.io\\/(.*)/\"\n\n    # the base entry point of the REST API\n    API_URL = \"https://api.osf.io/v2/registrations/\"\n\n    # the files and metadata about the dataset\n    API_URL_META = API_URL + \"{api_record_id}/files/osfstorage/?format=jsonapi\"\n    META_FILES_JSONPATH = \"data\"\n\n    # paths to file attributes\n    ATTR_FILE_LINK_JSONPATH = \"links.download\"\n    ATTR_NAME_JSONPATH = \"attributes.name\"\n    ATTR_SIZE_JSONPATH = \"attributes.size\"\n    ATTR_HASH_JSONPATH = \"attributes.extra.hashes.sha256\"\n    ATTR_HASH_TYPE_VALUE = \"sha256\"\n</code></pre> <ul> <li>The <code>API_URL</code> is the entry point for the URL. This URL serves the API.</li> <li>The <code>REGEXP_ID</code> is used to parse the URL and extract the ID. This ID is passed to the function <code>_get</code> with name <code>record_id</code>.</li> <li>Next, the metadata should be retrieved.</li> <li>For every file, download should be called.</li> </ul>"},{"location":"development/#datahugger-for-research-software","title":"Datahugger for research software","text":"<p>Scientific software rarely offers the options to import datasets from a DOI. Imagine what it would look like if you could. You can open a statistical software and you can start working on any published dataset. This is why we need persistent identifiers.</p>"},{"location":"download/","title":"Download dataset","text":"<p>The following example downloads dataset 10.5061/dryad.x3ffbg7m8 to the folder <code>data</code>.</p>"},{"location":"download/#download-dataset-from-doi","title":"Download dataset from DOI","text":"CLIPython <pre><code>datahugger 10.5061/dryad.31zcrjdm5 data\n</code></pre> <p>In some situations, you might have to quote the number the DOI (e.g. <code>datahugger \"10.5061/dryad.31zcrjdm5\" data</code>)</p> <pre><code>import datahugger\n\ndatahugger.get(\"10.5061/dryad.x3ffbg7m8\", \"data\")\n</code></pre>"},{"location":"download/#download-dataset-from-url","title":"Download dataset from URL","text":"CLIPython <pre><code>datahugger https://datadryad.org/stash/dataset/doi:10.5061/dryad.x3ffbg7m8 data\n</code></pre> <p>In some situations, you might have to quote the number the DOI (e.g. <code>datahugger \"https://datadryad.org/stash/dataset/doi:10.5061/dryad.x3ffbg7m8\" data</code>)</p> <pre><code>import datahugger\n\ndatahugger.get(\"https://datadryad.org/stash/dataset/doi:10.5061/dryad.x3ffbg7m8\", \"data\")\n</code></pre>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#is-supplementary-material-to-papers-supported","title":"Is supplementary material to papers supported?","text":"<p>At the moment not. There is a lack of standardisation among journals on supplementary material. As far as we know, there are no journal with proper APIs for supplementary material. Please let us know if there is a journal with a proper API.</p>"},{"location":"faq/#the-data-repository-is-not-supported-by-datahugger","title":"The data repository is not supported by Datahugger","text":""},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation-with-pip","title":"Installation with pip","text":"<p>Datahugger requires Python 3.6 or later.</p> <pre><code>pip install datahugger\n</code></pre>"},{"location":"installation/#upgrade","title":"Upgrade","text":"<pre><code>pip install --upgrade datahugger\n</code></pre>"},{"location":"installation/#extra-dependencies","title":"Extra dependencies","text":"<p>To download datasets from some repositories, you need addition requirements. When a service requires more dependencies, the installation instructions are given.</p> <p>To download all dependencies, use</p> <pre><code>pip install datahugger[all]\n</code></pre>"},{"location":"object/","title":"Datahugger result object","text":"<p>After you have downloaded the contents of a dataset, you can inspect the result of the download. Assign the returned object to a variable.</p> <p>This is Python API only.</p>"},{"location":"object/#scitree","title":"Scitree","text":"<p> Experimental</p> <p>The Datahugger result object has a method tree to print the tree. The tree is optimized for scientific use (see scitree and scisort).</p> Python <pre><code>import datahugger\n\ndh_data = datahugger.get(\"10.5061/dryad.x3ffbg7m8\", \"data\")\n\ndh_data.tree()\n</code></pre> <pre><code>data/\n\u251c\u2500\u2500 README_Pfaller_Robinson_2022_Global_Sea_Turtle_Epibiont_Database.txt\n\u2514\u2500\u2500 Pfaller_Robinson_2022_Global_Sea_Turtle_Epibiont_Database.csv\n\n0 directories, 2 files\nREADME Data Code\n</code></pre>"},{"location":"options/","title":"Options","text":""},{"location":"options/#skip-large-files","title":"Skip large files","text":"<p>For most repositories, it is possible to skip files that exceed a certain number of bytes. For example, you want to skip files larger than 50Mb.</p> CLIPython <pre><code>datahugger 10.5061/dryad.31zcrjdm5 data --max_file_size 50000000\n</code></pre> <pre><code>datahugger.get(\"10.5061/dryad.x3ffbg7m8\", \"data\", max_file_size=50000000)\n</code></pre>"},{"location":"options/#extract-single-zip","title":"Extract single zip","text":"<p>Some services like Zenodo don't offer an option to preserve folder structures. Therefore, the content is often zipped before being uploaded to the service. In this case, Datahugger will unzip the file to the output folder by default.</p> <p>Disable auto unzip function</p> CLIPython <p>Not available at the moment</p> <pre><code>datahugger.get(\"10.5061/dryad.x3ffbg7m8\", \"data\", auto_unzip=False)\n</code></pre>"},{"location":"options/#download-mode","title":"Download mode","text":"<p>By default, Datahugger skips the download of files and datasets that are already available on the local system. The options are: \"skip_if_exists\", \"force_redownload\".</p> CLIPython <pre><code>datahugger 10.5061/dryad.31zcrjdm5 data --download_mode force_redownload\n</code></pre> <pre><code>datahugger.get(\"10.5061/dryad.x3ffbg7m8\", \"data\", download_mode=\"force_redownload\")\n</code></pre>"},{"location":"options/#progress","title":"Progress","text":"<p>By default, Datahugger shows the download progress. You can disable the progress indicator.</p> CLIPython <pre><code>datahugger 10.5061/dryad.31zcrjdm5 data --no-progress\n</code></pre> <pre><code>datahugger.get(\"10.5061/dryad.x3ffbg7m8\", \"data\", progress=False)\n</code></pre>"},{"location":"repositories/","title":"Supported repositories","text":"<p>Datahugger offers support for more than 150 generic and specific (scientific) repositories (and more to come!).</p> <p></p> <p>We are still expanding Datahugger with support for more repositories. You can help by requesting support for a repository in the issue tracker. Pull Requests are very welcome as well.</p> <p>The following list gives an (non-exclusive) overview of repositories supported by Datahugger.</p>"},{"location":"repositories/#supported-repositories_1","title":"Supported repositories","text":""},{"location":"repositories/#single-implementations","title":"Single implementations","text":"<ul> <li>zenodo</li> <li>figshare</li> <li>GitHub</li> <li>DRYAD</li> <li>Hugging Face</li> <li>Open Science Framework (OSF)</li> <li>Mendeley Data</li> </ul>"},{"location":"repositories/#dataone-repositories","title":"DataOne repositories","text":"<p>The following repositories (non exhaustive) make use of DataOne software. DataOne software is supported by Datahugger.</p> <ul> <li>Arctic Data Center</li> <li>Knowledge Network for Biocomplexity (KNB)</li> <li>P\u00f4le National de Donn\u00e9es de Biodiversit\u00e9 Data Catalog</li> <li>California Ocean Protection Council</li> <li>portal.edirepository.org</li> <li>Gulf of Alaska Data Portal</li> <li>Partnership for Interdisciplinary Studies of Coastal Oceans</li> <li>Atmospheric Radiation Measurement (ARM)</li> <li>ScienceDB (Chinese Academy of Sciences)</li> <li>ESS-DIVE</li> <li>Hydroshare</li> <li>EarthChem</li> <li>IEDA2</li> <li>U.S. ANTARCTIC PROGRAM DATA CENTER</li> <li>Internation Year of the Salmon</li> <li>PANGAEA</li> <li>Rolling Deck to Repository (R2R)</li> <li>SEAD</li> </ul>"},{"location":"repositories/#dataverse-repositories","title":"DataVerse repositories","text":"<p>See https://dataverse.org/institutions and DataVerse on Re3data.org for an overview of DataVerse repositories.</p>"},{"location":"repositories_not_supported/","title":"Not supported repositories","text":""},{"location":"repositories_not_supported/#reasons-of-failure","title":"Reasons of failure","text":"<p>There are several reasons why Datahugger can't download the contents of a DOI. Besides internet and network errors, reasons for failure include (but not limited to):</p> <ul> <li>Not a valid DOI or URL</li> <li>DOI doesn't point to a data repository</li> <li>Data repository isn't in the list of supported repositories.</li> <li>DOI is no longer available in the repository.</li> </ul>"},{"location":"repositories_not_supported/#not-supported-error-message","title":"Not supported error message","text":"<p>When a repository is not supported, an error is returned (exit 1).</p> CLIPython <pre><code>datahugger 10.5061/dryad.31zcrjdm5 data\n</code></pre> <pre><code>Traceback (most recent call last):\n  File \"/Users/Bruin056/.pyenv/versions/sra-dev/bin/datahugger\", line 8, in &lt;module&gt;\n    sys.exit(main())\n  File \"/Users/Bruin056/Documents/GitHub/datahugger/datahugger/__main__.py\", line 65, in main\n    get(\n  File \"/Users/Bruin056/Documents/GitHub/datahugger/datahugger/api.py\", line 185, in load_repository\n    return get(\n  File \"/Users/Bruin056/Documents/GitHub/datahugger/datahugger/api.py\", line 197, in load_repository\n    service_class = _resolve_service(url, doi)\n  File \"/Users/Bruin056/Documents/GitHub/datahugger/datahugger/api.py\", line 226, in _resolve_service\n    service_class = _resolve_service_with_re3data(doi)\n  File \"/Users/Bruin056/Documents/GitHub/datahugger/datahugger/api.py\", line 246, in _resolve_service_with_re3data\n    publisher = get_datapublisher_from_doi(doi)\n  File \"/Users/Bruin056/Documents/GitHub/datahugger/datahugger/utils.py\", line 93, in get_datapublisher_from_doi\n    raise ValueError(\"DOI not found\")\nValueError: DOI not found\n</code></pre> <pre><code>datahugger.get(\"10.5061/dryad.x3ffbg7m8\", \"data\")\n</code></pre> <pre><code>Traceback (most recent call last):\n  File \"/Users/Bruin056/.pyenv/versions/sra-dev/bin/datahugger\", line 8, in &lt;module&gt;\n    sys.exit(main())\n  File \"/Users/Bruin056/Documents/GitHub/datahugger/datahugger/__main__.py\", line 65, in main\n    get(\n  File \"/Users/Bruin056/Documents/GitHub/datahugger/datahugger/api.py\", line 185, in load_repository\n    return get(\n  File \"/Users/Bruin056/Documents/GitHub/datahugger/datahugger/api.py\", line 197, in load_repository\n    service_class = _resolve_service(url, doi)\n  File \"/Users/Bruin056/Documents/GitHub/datahugger/datahugger/api.py\", line 226, in _resolve_service\n    service_class = _resolve_service_with_re3data(doi)\n  File \"/Users/Bruin056/Documents/GitHub/datahugger/datahugger/api.py\", line 246, in _resolve_service_with_re3data\n    publisher = get_datapublisher_from_doi(doi)\n  File \"/Users/Bruin056/Documents/GitHub/datahugger/datahugger/utils.py\", line 93, in get_datapublisher_from_doi\n    raise ValueError(\"DOI not found\")\nValueError: DOI not found\n</code></pre>"},{"location":"repositories_not_supported/#request-support","title":"Request support","text":"<p>If a repository is not supported by Datahugger, you can open an issue in the GitHub issue tracker.</p>"},{"location":"reproducible/","title":"Work reproducible with Datahugger","text":"<p>By downloading a dataset directly from a DOI, you can improve the reproducibility of your (scientific) work. It also saves manual work and time. Because of this, sharing work is easier. In the following sections, we discuss possible ways to integrate Datahugger in your workflow.</p>"},{"location":"reproducible/#publish-a-project-with-datahugger","title":"Publish a project with datahugger","text":""},{"location":"reproducible/#find-a-dataset","title":"Find a dataset","text":"<p>First, find a dataset you want to use in your analysis. You can use dataset search engines like Google Dataset Search or look for references in scientific publications. Once you found the dataset, copy the DOI. If there is no DOI available, use the URL to the dataset.</p>"},{"location":"reproducible/#instruct-user-to-install-datahugger","title":"Instruct user to install datahugger","text":"<p>If the user doesn't have datahugger installed on their device, it is required to install datahugger. Datahugger can be added to an existing Python installation file like <code>requirements.txt</code> of via documentation <code>pip install datahugger</code>.</p>"},{"location":"reproducible/#scenario-1-standalone-project-setup","title":"Scenario 1: Standalone project setup","text":"<p>In this scenario, you create a script or piece of documentation to setup the prerequirements for your project. This likely contains the installation dependencies and the download of the data with Datahugger. The following example shows an example for a Python project.</p> <pre><code>pip install -r requirements.txt\ndatahugger 10.xxx/yyy data\n</code></pre> <p>This script sets up the required Python dependencies and downloads the dataset.</p>"},{"location":"reproducible/#scenario-2-single-workflow","title":"Scenario 2: Single workflow","text":"<p>In this scenario, the data download is part of the same script or workflow as the analysis. This is common for interactive environments like Jupyter Notebooks.</p>"},{"location":"reproducible/#tips-for-git-users","title":"Tips for git users","text":""},{"location":"reproducible/#add-download-folder-to-gitignore","title":"Add download folder to <code>.gitignore</code>","text":"<p>Are you using git for version control? Add the download folder to the <code>.gitignore</code> file. This prevents you from adding the new dataset to the history. As you can redownload the same dataset, there is no need to track the dataset (it's disposable).</p>"},{"location":"reproducible/#download-without-progress-indicators","title":"Download without progress indicators","text":"<p>A redownload of the data will likely result in different progress output as download times will vary. For some applications, like Jupyter Notebooks, this will result in changes in the output, where there are no changes in the actual results. To prevent this, you can disable the progress indicator.</p> CLIPython <pre><code>datahugger 10.5061/dryad.31zcrjdm5 data --no-progress\n</code></pre> <pre><code>datahugger.get(\"10.5061/dryad.x3ffbg7m8\", \"data\", progress=False)\n</code></pre>"},{"location":"reproducible/#share-your-project","title":"Share your project","text":"<p>After publishing your project in a data repository or alike, others can start reusing it. Ideally, your downloaded data is not republished. This implies that the user of your project needs to download the assets with datahugger as well. This means that <code>datahugger</code> needs to be added to the installation dependencies.</p>"},{"location":"reproducible/#reuse-a-project-with-datahugger","title":"Reuse a project with datahugger","text":""},{"location":"reproducible/#install-datahugger","title":"Install datahugger","text":"<p>Ideally, the installation instructions of the project you want to (re)use provide installation instructions or automates the installation of dependencies. If not, please install datahugger.</p>"},{"location":"working/","title":"How Datahugger works","text":""},{"location":"working/#architecture","title":"Architecture","text":"<p>Datahugger solves both a conceptual and technical issue. The main challenge for datahugger is to connect the DOI to the machine-to-machine interface of the repository (the API). For humans, it is often clear to click the download button. However computers like to interact with the API. Unfortunately, there is no metadata describing the API entry point and protocol. Datahugger tries to solve this issue by creating this metadata or by doing an educated guess. The following flowchart provides an overview of the working of datahugger.</p> <p> </p>"},{"location":"working/#interesting-reads","title":"Interesting reads","text":"<p>The following articles and technical documents highlight relevant aspects of the DOI to data issue.</p> <ul> <li>Harvey MJ, Mason NJ, McLean A, Rzepa HS. Standards-based metadata procedures for retrieving data for display or mining utilizing persistent (data-DOI) identifiers. J Cheminform. 2015 Aug 8;7:37. doi: 10.1186/s13321-015-0081-7. PMID: 26257829; PMCID: PMC4528360.</li> <li>Sara Lafia &amp; Werner Kuhn (2018) Spatial Discovery of Linked Research Datasets and Documents at a Spatially Enabled Research Library, Journal of Map &amp; Geography Libraries, 14:1, 21-39, DOI: 10.1080/15420353.2018.1478923</li> <li>DOI Content Negotiation (Crosscite.org)</li> <li>Wass, Joe. \u201cURLs and DOIs: A Complicated Relationship.\u201d Crossref, https://www.crossref.org/blog/urls-and-dois-a-complicated-relationship/.</li> </ul>"}]}